---
title: "Progetto Inferenza Statistica"
output: rmarkdown::github_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(dplyr)
library( faraway )
library( leaps )
library(MASS)
library( GGally)
library(BAS)
library(rgl)
library(corrplot)
```

## Il Dataset

Prima di tutto definiamo la working directory:

`{setwd("C:/Users/alewi/Documents/University/HKUST & PoliMi/II Semestre/Inferenza Statistica/Progetto")}`

IMPORTANTE! Cambiare la directoy a seconda del pc.

Importiamo il Dataset, presente nella cartella `Dati/`:

```{r setup, include=FALSE}
setwd("C:/Users/alewi/Documents/University/HKUST & PoliMi/II Semestre/Inferenza Statistica/Progetto")
df <- read_excel("./Dati/Dropout20240226_IngMate.xlsx")
View(Data)
```

## Regressione Logistica

Consideriamo innanzitutto solo gli studenti con carriere terminate, cioÃ¨ o che si sono laureati o che hanno abbandonato il corso di studio:

```{r setup}
df$career_anonymous_id <- NULL
df$career_time <- NULL
df$stud_career_degree_start_id <- NULL

filtered_df <- df %>% filter(stud_career_status != 'A')
```

Selezioniamo dal dataset le variabili numeriche:

```{r setup}
numerical_vars <- sapply(filtered_df, is.numeric)  # Find numeric columns
numerical_df <- filtered_df[, numerical_vars]  # Subset dataframe with numeric columns

```

Osserviamo se esistono correlazioni significative fra i dati numerici:

```{r, echo=FALSE}
X = numerical_df[, -5]
corrplot(cor(X), method='color')
```

Effettuiamo la regressione logistica fra le variabili numeriche del dataset:

```{r}
# Create a formula for linear model
formula <- as.formula(paste("dropout ~", paste(names(numerical_df[,-which(names(numerical_df) == "dropout")]), collapse = " + ")))

# Fit the linear model
model <- glm(formula, data = numerical_df)

# Print the summary of the model
summary(model)
```

Cerchiamo di trovare il miglior modello con un Automatic Selection Method:
```{r}
x = model.matrix( model ) [ , -1 ]
y = na.omit(numerical_df)$dropout

adjr = leaps( x, y, method = "adjr2" )
names(adjr)
adjr

bestmodel_adjr2_ind = which.max( adjr$adjr2 )
adjr$which[ bestmodel_adjr2_ind, ] 

maxadjr(adjr,15 )
```
Decidiamo di utilizzare il modello numero 11, dal momento che presenta 6 features, perdendo solamente 0.002 di adjr2 rispetto al caso ottimale. 

Il modello diventa dunque: 

```{r}
# Assuming 'df' is your dataframe and 'target' is your target variable
# Select only the columns you're interested in
selected_df <- numerical_df[, c(1,3,4,5,6,9,10)]

# Create a formula for the model
# This assumes that the first column is the target variable
formula <- as.formula(paste("dropout ~", paste(names(selected_df[,-which(names(selected_df) == "dropout")]), collapse = " + ")))

# Fit the model
model_opt <- glm(formula, data = selected_df, family = binomial)

# Print the summary of the model
summary(model_opt)
```
Applichiamo adesso il modello lineare sugli studenti che non hanno ancora completato il loro percorso universitario: 

```{r}
new_data = filtered_df <- df %>% filter(stud_career_status == 'A')
new_data_vars <- sapply(new_data, is.numeric)  # Find numeric columns
new_data <- new_data[, numerical_vars]  # Subset dataframe with numeric columns
new_data = new_data[,c(-4,-8)]
new_data = na.omit(new_data)

View(new_data)

predicted_values <- predict(model_opt, newdata = new_data, type = "response")
plot(new_data$career_time_conv, predicted_values)
```
## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
